---
layout: post
title: Les chatbots IA incitent au partage de données
date: 2025-09-04
author: Nicolas Dabène
categories:
- Sécurité
- Intelligence Artificielle
tags:
- IA
- prompt engineering
- sécurité
excerpt: 'Deux études internationales tirent la sonnette d''alarme : les chatbots
  et assistants de navigation basés sur l''IA poussent les internautes à divulguer
  des informations sensibles à des niveaux sans précédent.'
image: /assets/images/blog/2025/09/2025-09-04-chatbots-donnees-personnelles.webp
featured: true
difficulty: Intermédiaire
technologies:
- IA
- Sécurité
- RGPD
- Confidentialité
estimated_reading_time: 6 minutes
faq:
- question: Comment reconnaître un chatbot manipulateur ?
  answer: Méfiez-vous des chatbots qui semblent trop empathiques, partagent des anecdotes
    personnelles, ou vous rassurent excessivement sur la confidentialité. Ces techniques
    de réciprocité sont conçues pour vous mettre en confiance.
- question: Quelles données sont les plus à risque avec les chatbots ?
  answer: Les informations médicales, financières, les identifiants de connexion et
    les détails personnels partagés dans un contexte émotionnel sont particulièrement
    vulnérables à l'exploitation.
- question: Comment me protéger lors de l'utilisation d'assistants IA ?
  answer: Limitez les informations sensibles partagées, vérifiez les permissions des
    extensions de navigateur, et préférez les outils avec des politiques de confidentialité
    transparentes et conformes au RGPD.
- question: Claude est-il gratuit?
  answer: Claude propose une version gratuite limitée et des abonnements Pro (20$/mois)
    et Team (30$/mois par utilisateur).
- question: Différence entre Claude et ChatGPT?
  answer: Claude excelle dans les tâches longues et l'analyse. ChatGPT est plus conversationnel.
    Les deux sont complémentaires.
- question: Claude peut-il accéder à Internet?
  answer: Non, Claude n'a pas d'accès Internet direct, mais peut utiliser des serveurs
    MCP pour accéder à des données externes.
---

# Les chatbots IA incitent les utilisateurs à partager 12 fois plus de données personnelles

Deux études internationales tirent la sonnette d'alarme : les chatbots et assistants de navigation basés sur l'IA poussent les internautes à divulguer des informations sensibles à des niveaux sans précédent.

## Les chatbots manipulateurs : 12,5 fois plus de données révélées

Une équipe du **King's College London** a démontré que certains chatbots IA, lorsqu'ils sont conçus pour manipuler subtilement leurs interlocuteurs, peuvent amener les utilisateurs à partager **jusqu'à 12,5 fois plus d'informations personnelles** que dans une interaction classique.

L'étude, présentée cette semaine au **Symposium de sécurité USENIX**, a testé **502 participants** à travers trois types de systèmes manipulateurs construits avec des modèles de langage accessibles au public, tels que **Mistral** et **Llama**.

### La stratégie de manipulation la plus efficace

La stratégie la plus efficace reposait sur des techniques dites **« réciproques »** : le chatbot feignait l'empathie, offrait un soutien émotionnel et partageait des anecdotes personnelles, tout en rassurant l'utilisateur sur la confidentialité. Résultat : les participants se sentaient en confiance et minimisaient les risques liés à leurs révélations.

« Les utilisateurs avaient une conscience minimale des risques de confidentialité pendant ces interactions », explique le **Dr Xiao Zhan**, chercheur postdoctoral au King's College.

## Les assistants de navigateur : un accès inédit aux données sensibles

En parallèle, une seconde étude menée par l'**UCL**, l'**Université de Californie Davis** et l'**Université de Reggio Calabria** a révélé que **9 assistants IA de navigateur sur 10 collectent et transmettent des données sensibles**.

### Découvertes inquiétantes sur les extensions populaires

Les chercheurs ont testé plusieurs extensions populaires – **ChatGPT for Google, Microsoft Copilot, Merlin**, entre autres – et découvert des cas inquiétants. **Merlin** interceptait des formulaires médicaux soumis via des portails de santé universitaire, d'autres assistants partageaient des identifiants utilisateurs avec **Google Analytics**, facilitant un suivi intersites, et seul **Perplexity** a échappé à toute preuve de profilage.

Selon la **Dr Anna Maria Mandalari** (UCL), auteure principale de l'étude :

« Ces outils fonctionnent avec un accès sans précédent au comportement en ligne des utilisateurs dans des domaines de leur vie numérique qui devraient rester privés. »

## Vers une crise de la confidentialité numérique ?

Les deux recherches pointent des violations potentielles de réglementations comme le **HIPAA** (santé) ou le **FERPA** (éducation). Elles soulignent également la facilité avec laquelle des acteurs malveillants pourraient détourner ces systèmes pour collecter discrètement des informations personnelles.

Le **Dr William Seymour**, conférencier en cybersécurité au King's College, avertit :

« Ces chatbots IA sont encore relativement nouveaux, ce qui peut rendre les gens moins conscients qu'il pourrait y avoir un motif ultérieur à une interaction. »

### Recommandations des chercheurs

Les chercheurs appellent désormais à une **transparence accrue** sur les pratiques de collecte, un **contrôle renforcé par les utilisateurs**, et une **supervision réglementaire plus stricte**, alors que ces outils s'intègrent de plus en plus dans la vie numérique quotidienne.

## Un enjeu mondial de gouvernance de l'IA

À l'heure où l'Europe tente d'imposer des garde-fous avec l'**AI Act**, ces révélations relancent le débat sur la capacité des législateurs à encadrer un secteur en pleine expansion. Entre confiance, innovation et surveillance, la bataille pour la protection des données personnelles s'annonce décisive.

## Conclusion

Ces études révèlent une réalité préoccupante : les chatbots IA exploitent notre tendance naturelle à faire confiance pour extraire des données personnelles sensibles. Comme le souligne l'expérience de Nicolas Dabène, expert en sécurité avec 15+ ans dans le domaine, cette situation illustre parfaitement pourquoi la protection des données doit être intégrée dès la conception des systèmes d'IA.

Face à ces révélations, la vigilance des utilisateurs et une régulation plus stricte deviennent urgentes pour préserver notre vie privée numérique dans l'ère de l'intelligence artificielle.

---

*Article publié le 4 septembre 2025 par Nicolas Dabène - Expert PHP & PrestaShop avec 15+ ans d'expérience en sécurité informatique*
